% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mixed.dens.R
\name{cmdSuperLearner}
\alias{cmdSuperLearner}
\title{SuperLearner-based estimation of (c)onditional (m)ixed continuous-discrete (d)ensity function}
\usage{
cmdSuperLearner(A, W, control = list(), cvControl = list())
}
\arguments{
\item{A}{\code{n x 1} numeric vector of exposure values.}

\item{W}{\code{n x p} data.frame of covariate values to condition upon.}

\item{control}{Optional list of control parameters. See \code{\link{cmdSuperLearner.control}} for details.}

\item{cvControl}{Optional list of control parameters for cross-validation. See \code{\link{cmdSuperLearner.cvControl}} for details.}
}
\value{
\code{cmdSuperLearner} returns a named list with the following elements:
\item{fits}{A list of fits for each of the number of bins specified in control$n.bins, as output by \link{cmdSuperLearner.onebin.}}
\item{cv.library.densities}{Cross-validated densities from every element of the library.}
\item{library.densities}{Densities predicted using the full data.}
\item{SL.densities}{Super learner densities predicted on the full data.}
\item{coef}{The coefficient of the meta-learner.}
\item{library.names}{Names of library algortihms.}
\item{a.ecdf}{Empirical CDF of the exposure.}
\item{control}{Control elements used in fitting.}
\item{cvControl}{Cross-validation controls used in fitting.}
}
\description{
This function estimates a standardized conditional density function that may have both continuous and discrete components. Let \code{A} be a univariate exposure and \code{W} be a p-dimensional vector of covariates. Then this function estimates p(a | w) / p(a) at points  of absolute continuity of the marginal distribution of \code{A}, where p(a | w) = (d/da)P(A <= a | W = w) is the conditional density of \code{A} given \code{W = w} evaluated at a and p(a) = (d/da) P(A <= a) is the marginal density of \code{A}, and at discrete points of the marginal distribution of \code{A}, this function estimates P(A = a | W = w)/P(A = a).
}
\details{
The basic idea is to first transform A by its empirical CDF to obtain U = F_n(A), because the conditional density or  mass function of F(A) equals the standardized conditional density/mass of A for F(a) = P(A <= a). Then, the support [0,1] of U is discretized into \code{b} sets (which may be singleton sets) using the marginal distribution of U. Within each of these sets, the conditional probability that U falls in the set given \code{W} is estimated using the specified wrapper algorithms from the \code{\link[SuperLearner]{SuperLearner}} package. This procedure is repeated over a set of possible number of bins \code{b}, and optimal weights for all algorithms are found using negative log likelihood loss.
}
\examples{
# Sample data
n <- 1000
W <- data.frame(W1 = runif(n))
Z <- rbinom(n, size = 1, prob = 1/(1 + exp(2-W$W1)))
A <- (1-Z) * rnorm(n, mean = W$W1, sd = abs(1 + W$W1))
fit <- cmdSuperLearner(A, W, control=list(SL.library = c("SL.mean", "SL.glm", "SL.gam", "SL.earth"), verbose=TRUE, n.bins = c(2:10)))
}
